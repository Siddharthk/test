

\section{Independent Component Analysis (ICA) Model}
\label{sec:format}
 

\begin{figure}[t]
\centering
\subfigure{
\includegraphics[height=3.8in,width=5.9in]{chap4/res_7/framework.eps}
}
\caption
{ICA Model - IC1: Independent component containing the foreground text, IC2: Independent
component containing the Background, IC3: Independent component containing the mixture of foreground and background.
y $\in$ $\{$R,G,B$\}$}
\label{fig:frame}
\end{figure}
Independent Component Analysis (ICA) has been an active research topic 
because of its potential applications in signal processing. And recently 
it has received attention in image processing tasks.
In ICA model, more than one observation signals are needed to achieve the 
analysis.
Currently when ICA is applied in image processing, images are divided into blocks \cite{chap4-1,chap4-2,chap4-3}  
with size of 8x8 or 16x16. These blocks are taken as the observations of ICA model.
But here we divide the image into sub-components i.e Red, Green and blue channels which are
taken as the observation of ICA model (Fig \ref{fig:frame}). 

The goal of ICA is to separate independent 
source signals from the observed signals, which is assumed 
to be the linear mixtures of independent source components. 
The mathematical model of ICA is formulated by 
mixture processing and an explicit decomposition processing.  
Assume there exists a set of `n' unknown source signals 
$S=\{s_1, s_2,..., s_n \}$. The assumptions of the components 
$\{s_i\}$  include mutual independence, stationary and zero mean. 
A set of observed signals $X=\{x_1,x_2,...,x_n\}$, are regarded as the mixture of the source 
components. The most frequently considered 
mixing model is the linear instantaneous noise free model,
which is described as: 
\begin{equation}
x_i=\sum_{j=1}^{n}a_{ij}s_j 
\end{equation}or in the matrix notation
\begin{equation}
X=A.S  
\end{equation}where A is an unknown full rank mixing matrix, which is 
also called mixture matrix. Eqn 4.1 assumes that there exists a 
linear relationship between the sources $S$ and the observations 
$X$. The ICA model describes how the observed data is 
generated by a process of mixing the components. In our case, `n' is equal to 3. 


\section{Natural Scene Text Binarization}
Binarization is a process to convert a grayscale or color 
images into black and white images. Binarization 
problem classifies individual pixels as foreground text or 
background. A pixel is grouped into two colors i.e black and white. The black pixels represent the 
foreground text while the white pixels represent the background and thus a binary image is created.
It is a necessary step before recognition of text by OCR (Optical Character Recognition). 
Accurate segmentation of natural scene texts can significantly increase the success of
the subsequent text recognition step. 
But these scene texts contain numerous degradations 
such as noise, uneven
illumination, blur, highlights, shadows,  multiple colors and complex textured background.
These issues severely degrade the segmentation accuracy. Hence we propose a technique which helps is 
segmentation of text from complex backgrounds.
Fig \ref{fig:sample} shows the sample images that we considered in this work.


% Understanding texts from natural scenes/scene text image 
% such as, commercial signboards, traffic signs, and advertising 
% billboards is very useful in many purposes such as assistant 
% system for impaired persons, drawing attention of a driver to 
% traffic signs, text translation system for foreigners, potential 
% applications like license plate recognition, digital note taking, 
% document archiving and wearable computing. Binarization 
% problem concerns classifying individual pixels as text or 
% background. Binarization is necessary to bridge the gap 
% between localization and recognition by OCR. The output of 
% this step is a binary image where black text characters appear 
% on a white background. Current techniques are categorized into 
% two groups: global binarization and local binarization or 
% adaptive binarization.
% 
% Natural scene texts contain numerous degradations not
% usually present in machine printed ones such as uneven
% lighting, blur, complex background, perspective distortion,
% multiple colours etc
% % Ctrl - D comments multiple lines & Ctrl - shift - D uncomments it.
% In this paper we describe a new algorithm for scene text
% segmentation by exploiting inverse rendering, i.e. decomposing
% an input image into basic rendering elements. This technique
% assigns each pixel in the image to a foreground (text) layer
% or a background layer. Previous work [1] has shown that
% accurate segmentation can significantly increase the success of
% the subsequent text recognition step. However, segmentation in
% natural images i because of effects such
% as highlights, reflections, shadows, noise and focal blur which can severely degrade the
% segmentation accuracy.
% Binarization is a process to convert a color or gray scale 
% image into blank and white image. It is done to divide the 
% image into foreground and background pixels so that text 
% extraction can easily perform.  
%In the recent years, content-based image analysis techniques have received more attention with the advent of various digital image capture devices.
%The images captured by these devices can vary dramatically depending on lighting conditions, reflections, shadows and specularities.
%These images contain numerous degradations such as uneven lighting, complex background, multiple colours, blur etc.
%We propose a method for removing reflections, shadows and specularities in natural scene text images and extracting out the text from a single image. 
	
%Binarization method is one of the important pre-processing steps in document image analysis system. 
%It directly affects the performance of the subsequent steps like character/word segmentation and recognition.
%Binarization of text can be defined as classifying individual pixels as foreground (text) or background. 

%There are many algorithms that aim at extracting foreground text from background in images but thresholding remains one of
%the oldest form that is used in many image processing applications. Many sophisticated approaches often have thresholding as a pre-processing step. 
%It is often used to segment images consisting of bright objects against dark backgrounds or vice versa \cite{A1,A3,A4}.
%It typically works well for images where the foreground and background are clearly defined.
%For color thresholding images, most algorithms convert the 
%RGB image into grayscale but here we will make use of the RGB channel as three different sources. 

%Traditional thresholding based binarization can be grouped into two categories: the one which uses global
%threshold for the given images like Otsu \cite{A2}, Kittler {\em et al}. 
%\cite{A5} and the one with local thresholds like Sauvola \cite{A6},
%Niblack \cite{A9}. In global thresholding methods \cite{A2,A7}, global thresholds are
%used for all pixels in image. These methods are fast and robust as
%they use a single threshold based on the global histogram of the gray-value pixels of the image.
%But they are not suitable for complex
%and degraded scene images. 
%Also selecting the right threshold for the whole image is usually a challenge 
%because it is difficult for the thresholding
%algorithm to differentiate foreground text from complex background.
\begin{figure}[t]
\centering
\subfigure[]{
\includegraphics[height=.6in,width=1.8in]{results/res_3/1.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_3/2.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_3/6.eps}
\label{fig:subfig11}
}
\subfigure[]{
\includegraphics[height=.6in,width=1.8in]{results/res_3/3.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_3/4.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_3/8.eps}
\label{fig:subfig12}
}
\subfigure[]{
\includegraphics[height=.6in,width=1.8in]{results/res_3/5.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_3/7.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_3/10.eps}
\label{fig:subfig13}
}
\caption
{Some sample word images we considered in this work containing (a) reflective (b) shadowed and (c) specular background}
\label{fig:sample}
\end{figure}
\begin{figure}[t]
\centering
\subfigure{
\includegraphics[height=1.9in,width=4in]{results/framework.eps}

\label{fig:subfig11}
}
\caption
{Framework for the proposed method}
\label{fig:3}
\end{figure}


%On the other hand, local or adaptive binarization \cite{A8} methods changes the threshold over the image according to local region properties.
%Adaptive thresholding addresses variations in local intensities throughout the image.
%In these methods, a per-pixel threshold is computed based on a local window around each pixel. 
%Thus, different threshold values are used for different parts of the image. 
%These methods are proposed to overcome global binarization drawbacks but they can be sensitive
%to image artifacts found in natural scene text images like shadows, specularities and reflections.
%Mishra {\em et al}~\cite{A16} has recently formulated the problem of binarization as an MRF optimization problem. 
%The method shows superior performance over traditional binarization methods on many images, and we use it as the
%basis for our comparisons. However, their method is sensitive to the initial auto seeding process.
%On the other hand, we propose a method that removes shadows, specularity and reflections and thus produces a clean 
%binary images even for the images with complex background.
\begin{figure*}[t]
\centering
\subfigure[]{
\includegraphics[height=.6in,width=1.8in]{results/res_5/1.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_5/2.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_5/3.eps}
\label{fig:subfig11}
}
\subfigure[]{
\includegraphics[height=.6in,width=1.8in]{results/res_5/4.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_5/5.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_5/6.eps}
\label{fig:subfig12}
}
\subfigure[]{
\includegraphics[height=.6in,width=1.8in]{results/res_5/7.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_5/8.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_5/9.eps}
\label{fig:subfig12}
}
\caption
{Foreground and Background Extracted: (a) Shadowed background and foreground text
(b) Reflective background and foreground text (c) Specular background and foreground text }
\label{fig:extract}
\end{figure*}


%The primary issue related to binarizing text from scene images is the presence of complex/textured background. 
%When the background is uneven as a result of poor or non-uniform lighting conditions, the image will not be 
%segmented correctly by a fixed gray-level threshold. These complex background vary dramatically depending on
%lighting, specularities, reflections and shadows. The above methods applied directly to such images
%give poor results and cannot be used in OCR systems. In this paper, 
%we do an ICA based decomposition which enables us to separate text from complex backgrounds containing, reflections,
%shadows and specularities. 
%For binarization, we apply a global thresholding method on the independent components of the image
%and that with maximum textual properties is used for extracting the foreground text. Binarization results show 
%significant improvement in the extraction of text over other methods. Some of the word images that we used 
%for experiments are shown in Fig \ref{fig:1}.

%The remainder of the paper is organized as follows. We discuss the general ICA model in Section 2 followed by
%the detailed binarization process in section 3. We then show the results of the proposed method on a variety
%of images from the ICDAR dataset, followed by the conclusions and potential directions for further improvement.

% In the recent years, content-based image analysis techniques have received more
% attention with the advent of various digital image capture devices.
% The images captured by these devices can vary dramatically depending on 
% lighting conditions, reflections, shadows and specularities.
% These images contain numerous degradations such as uneven
% lighting, complex background,
% multiple colours, blur etc.
% We propose a method for removing reflections, shadows and specularities in natural scene text images
% and extracting out the text from a single image. 
% 
% Binarization method is one of the important pre-processing steps in
% document image analysis system. 
% It directly affects the performance of the
% subsequent steps like character/word segmentation and recognition.
% Binarization of text can be defined as classifying individual pixels as foreground (text) or
% background. 
% 
% There are many algorithms that aim at extracting foreground 
% (text) from background in images 
% but thresholding remains one of the oldest form that is used
% in many image processing applications. Many sophisticated approaches 
% often have thresholding as a pre-processing step. 
% It is often used to segment images consisting 
% of bright objects against dark backgrounds or vice versa \cite{A1,A3,A4}.
% It typically works well for images where the foreground and background are clearly defined.
% For color thresholding images, most algorithms convert the 
% RGB image into grayscale but here we will make use of the RGB channel as three different sources. 
% 
% Traditional thresholding based binarization can be grouped into two categories: the one which uses global
% threshold for the given images like Otsu \cite{A2}, Kittler {\em et al}. 
% \cite{A5} and the one with local thresholds like Sauvola \cite{A6},
% Niblack \cite{A9}. In global thresholding methods \cite{A2,A7}, global thresholds are
% used for all pixels in image. These methods are fast and robust as
% they use a single threshold based on the global histogram of the gray-value pixels of the image.
% But they are not suitable for complex
% and degraded scene images. 
% Also selecting the right threshold for the whole image is usually a challenge 
% because it is difficult for the thresholding
% algorithm to differentiate foreground text from complex background.
% \begin{figure}[t]
% \centering
% \subfigure[]{
% \includegraphics[height=.6in,width=1.6in]{results/res_3/1.eps}
% \includegraphics[height=.6in,width=1.6in]{results/res_3/2.eps}
% \label{fig:subfig11}
% }
% \subfigure[]{
% \includegraphics[height=.6in,width=1.6in]{results/res_3/3.eps}
% \includegraphics[height=.6in,width=1.6in]{results/res_3/4.eps}
% \label{fig:subfig12}
% }
% \subfigure[]{
% \includegraphics[height=.6in,width=1.6in]{results/res_3/5.eps}
% \includegraphics[height=.6in,width=1.6in]{results/res_3/6.eps}
% \label{fig:subfig13}
% }
% \caption[]
% {Some sample word images we considered in this work containing (a) reflective (b) shadowed and (c) specular background}
% \label{fig:1}
% \end{figure}
% On the other hand, local or adaptive binarization \cite{A8}
% methods changes the threshold over the image
% according to local region properties.
% Adaptive thresholding addresses variations in
% local intensities throughout the image.
% In these methods, a per-pixel threshold is computed
% based on a local window around each pixel. Thus, different
% threshold values are used for different parts of the image. These
% methods are proposed to overcome global binarization drawbacks
% but they can be sensitive
% to image artifacts found in natural scene text images like shadows, specularities and reflections.
% The recently published work by Mishra \emph{et al} \cite{A16} address the problem
% of binarization as an MRF optimization problem. The method shows superior
% performance over traditional binarization methods on many images, but is sensitive to initial
% auto seeding process. On the other hand, we propose a method which removes
% shadows, specularity and reflections and thus produces a clean binary images
% even for the images with complex background.
% 
% \begin{figure*}[t]
% \centering
% \subfigure[]{
% \includegraphics[height=.7in,width=2.0in]{results/res_5/1.eps}
% \includegraphics[height=.7in,width=2.0in]{results/res_5/2.eps}
% \includegraphics[height=.7in,width=2.0in]{results/res_5/3.eps}
% \label{fig:subfig11}
% }
% \subfigure[]{
% \includegraphics[height=.7in,width=2.0in]{results/res_5/4.eps}
% \includegraphics[height=.7in,width=2.0in]{results/res_5/5.eps}
% \includegraphics[height=.7in,width=2.0in]{results/res_5/6.eps}
% \label{fig:subfig12}
% }
% \subfigure[]{
% \includegraphics[height=.7in,width=2.0in]{results/res_5/7.eps}
% \includegraphics[height=.7in,width=2.0in]{results/res_5/8.eps}
% \includegraphics[height=.7in,width=2.0in]{results/res_5/9.eps}
% \label{fig:subfig12}
% }
% \caption[]
% {Foreground and Background Extracted: (a) Shadowed background and foreground text
% (b) Reflective background and foreground text (c) Specular background and foreground text }
% \label{fig:2}
% \end{figure*}
% 
% The issue related to binarizing text from
% images is the presence of complex/textured background. 
% When the background is uneven as a result of poor or
% non-uniform lighting conditions, the image will not be segmented correctly by a fixed gray-level
% threshold. These complex background
% vary dramatically depending on lighting, specularities, reflections and shadows.
% The above methods applied directly to these images give poor results and cannot be used 
% in OCR systems. In this paper, an ICA based approach is used as a preprocessing step to 
% extract foreground text from complex backgrounds containing reflections.
% For binarization, we apply a global thresholding method
% on the independent components containing maximum information about the
% foreground text. Binarization results show 
% great improvement in the extraction of text over other methods.  
% Some of the word images that we used for experiments are shown in Fig \ref{fig:1}
% 
% The remainder of the paper is organized as follows. We discuss the general ICA model in
% Section 2. In section 3, we show the detailed binarization process. In section 4,
% experimental results on ICDAR dataset is reported. We finally conclude the
% work in section 5.




%The objective of ICA is to estimate the source signals by 
%finding an decomposition matrix $H$, which is a permuted version of the inverse of the mixing matrix. 
%The separating procedure 
%for estimating the underlying source signals is:
%\begin{equation}
%Y=H.X 
%\end{equation}
%Therefore we find a linear representation of the observed signals, 
%which can satisfy the assumption of statistical independence. 
%But the obtained linear representation cannot 
%mathematically achieve statistical independence. Thus we
%find components that are as independent as possible. These components are only an 
%approximate of the real source signals.
%\begin{equation}
% Y\approx S
%\end{equation}



\subsection{Binarization process}
\begin{figure*}[t]
\centering
\subfigure[Input]{
\includegraphics[height=.6in, width=1.4in]{results/IC/orig.eps}
\label{fig:subfig11}
}
\subfigure[Red]{
\includegraphics[height=.6in, width=1.4in]{results/IC/R.eps}
\label{fig:subfig12}
}
\subfigure[$IC_1$]{
\includegraphics[height=.6in, width=1.4in]{results/IC/14_1.eps}
\label{fig:subfig11}
}
\subfigure[Binarized $IC_1$]{
\includegraphics[height=.6in, width=1.4in]{results/IC/14_1_binary.eps}
\label{fig:subfig13}
}\\
\hspace*{1.5in}
\subfigure[Green]{
\includegraphics[height=.6in, width=1.4in]{results/IC/G.eps}
\label{fig:subfig13}
}
\subfigure[$IC_2$]{
\includegraphics[height=.6in, width=1.4in]{results/IC/14_2.eps}
\label{fig:subfig12}
}
\subfigure[Binarized $IC_2$]{
\includegraphics[height=.6in, width=1.4in]{results/IC/14_2_binary.eps}
\label{fig:subfig13}
}\\
\hspace*{1.5in}
\subfigure[Blue]{
\includegraphics[height=.6in, width=1.4in]{results/IC/B.eps}
\label{fig:subfig14} } 
\subfigure[$IC_3$]{
\includegraphics[height=.6in, width=1.4in]{results/IC/14.eps}
\label{fig:subfig13}
}
\subfigure[Binarized $IC_3$]{
\includegraphics[height=.6in, width=1.4in]{results/IC/binary.eps}
\label{fig:subfig14} } 
\caption
{(a) Original word image (b),(e),(h) R, G and B channel respectively
(c),(f),(i) Independent Components, (d),(g),(j) Binarized image}
\label{fig:4}
\end{figure*}

A wide variety of ICA algorithms are available in the literature \cite{A11,A12}. These algorithms differ from each other on the
basis of the choice of objective function and selected optimization scheme. Here we use a fast fixed point
ICA algorithm to separate out the text from complex background in images. A Blind Source 
Separation method based on SVD \cite{A10} can also be used. Fig. \ref{fig:3} 
shows the complete framework for the proposed method.

\subsubsection{The Separation Model}

Consider the text image as a mixture of pixels from three different sources
and assume it to be a noiseless instantaneous mixture.
We use a single image i.e its R, G and B channels as three observed signals.
Therefore, we can define that the color intensity at each pixel from these three observed signals
mix linearly to give the resultant color intensity at that pixel.
Denoting these mixture images in row vector form
as $x_r$, $x_g$ and $x_b$, the linear mixing of the sources at a particular pixel $k$ can be
expressed in matrix form as follows:

\begin{equation}
%\resizebox{.91\hsize}{!}{$
\underbrace{\left[ {\begin{array}{c}
{x_r(k)} \\
{x_g(k)} \\
{x_b(k)}
 \end{array} } \right]}_{X}
=
\underbrace{\left[ {\begin{array}{ccc}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33} \\
 \end{array} } \right]}_{A}
\underbrace{\left[ {\begin{array}{c}
s_1(k)\\s_2(k)\\ s_3(k)
 \end{array} } \right]}_{S}
%$}
\end{equation} where $X$ is an instantaneous linear mixture of source
images at pixel $k$, A is the instantaneous $3$x$3$ square mixing matrix and S
is the source images which add up to form the color intensity 
at pixel $k$.  
The mixed images
in $X$ contain a linear combination of the source
images in $S$. We find the mixing matrix A and sources S using fixed point ICA
algorithm.
Derivation of the algorithm is beyond the scope of this paper.
The reader is encouraged to refer \cite{A11} for this. We summarize the 
fixed point ICA method in Algorithm 2.

\begin{algorithm}[t]
\caption{Fixed Point ICA}
\begin{algorithmic}[1] 
\REQUIRE $X$
\STATE Random initialization of $A$
\STATE $S=A^TX$
\STATE $A^+=Xg(S)^T$
where $g(x)=tanh(x)$ 
\STATE $A=A^+/||A^+||$  
\STATE If not converged, go back to 2.
\ENSURE $A,S$ 
\end{algorithmic}
\end{algorithm} 
%The amount of information contained in each independent component can be visually seen.
From this step, we get three independent sources or components.
Fig. \ref{fig:extract} shows the background and the foreground extracted.
%The independent component containing the maximum information about the foreground text
%is used for thresholding.
The resultant independent components for a particular word image can be seen in 
%\ref{fig:3}.
Fig. \ref{fig:4} which shows the independent component free from reflective background 
and containing maximum information of the foreground text.

\subsubsection{Thresholding}

Otsu thresholding \cite{A2} is a well-known algorithm that
determines a global threshold for an image by minimizing 
the within-class variance for the resulting classes (foreground pixels 
and background pixels). This is
done by equivalently maximizing the between-class variance
$\sigma _{B}^{2}(T)$ for a given threshold T:
\begin{equation}
\sigma_{B}^{2}=\alpha_1(T)\alpha_2(T)[\mu_1(T)-\mu_2(T)]^2 
\end{equation}
where $\alpha_i$ denotes the number of pixels in each class, $\mu_i$ denotes 
the mean of each class, and T is the value of the potential threshold. 
We apply this thresholding algorithm on all the three independent components to get the binarized image (Fig. \ref{fig:4}).
We can also apply Kittler \cite{A5} algorithm which is also a global thresholding method.
\begin{figure}[tp]
\centering
\subfigure[]{
\includegraphics[height=.6in,width=2in]{results/res_4/orig.eps}
}
\subfigure[]{
\includegraphics[height=.6in,width=2in]{results/res_4/ic.eps}
}
\subfigure[]{
\includegraphics[height=.6in,width=2in]{results/res_4/otsu.eps}
}
\subfigure[]{
\includegraphics[height=.6in,width=2in]{results/res_4/nib.eps}
\label{fig:subfig12}
}
\caption
{(a) Text containing specular highlight (b) IC (c) Otsu (d) Niblack}
\label{fig:5}
\end{figure}

%or we can do like this (at the top of the figure write the caption then it will be named as a,b,c.


\begin{figure*}[t]
\centering
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res5/orig.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res5/kit.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res5/nib.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res5/otsu.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res5/sau.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res6/orig.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res6/kit.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res6/nib.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res6/otsu.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res6/sau.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res7/orig.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res7/kit.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res7/nib.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res7/otsu.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res7/sau.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res8/orig.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res8/kit.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res8/nib.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res8/otsu.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res8/sau.eps}
}
\caption
{Failure cases for thresholding based methods. 
From left to right (a) Text Image (b) kittler (c) Niblack (d) Otsu (e) Sauvola}
\label{fig:bad}
%\end{figure*}
%\begin{figure*}[p]
\vspace{10mm}
\centering
\subfigure[]{
\includegraphics[height=.5in,width=1in]{chap4/ground/7.eps}
\includegraphics[height=.5in,width=1in]{chap4/ground/8.eps}
\includegraphics[height=.5in,width=1in]{chap4/ground/9.eps}
\includegraphics[height=.5in,width=1in]{chap4/ground/10.eps}
\includegraphics[height=.5in,width=1in]{chap4/ground/11.eps}
}
\subfigure[]{
\includegraphics[height=.5in,width=1in]{chap4/ground/1.eps}
\includegraphics[height=.5in,width=1in]{chap4/ground/2.eps}
\includegraphics[height=.5in,width=1in]{chap4/ground/3.eps}
\includegraphics[height=.5in,width=1in]{chap4/ground/4.eps}
\includegraphics[height=.5in,width=1in]{chap4/ground/5.eps}
}
%\label{fig:subfig11}
\caption
{(a) Scene Text Image (b) Ground Truth Binary Image}
\label{fig:ground}
\end{figure*}



%\begin{figure*}[hbp]
%\centering

%\label{fig:7}
%\end{figure*}

%\begin{figure*}[hbp]
%\centering

%\label{fig:7}
%\end{figure*}








To find the IC that contains the foreground text, we examine the connected components (CC) in the 
binarization of each IC. For each binarized image, we extract the following features from the CCs: 
average aspect ratio, variance of  CC size, and the deviation from linearity of their centroids.
A simple linear classifier is designed to separate the text and non-text classes in the above feature space.
After binarization, we identify the
connected components and remove non-text portions based on size and aspect ratio.

In some cases where the text image is severely degraded and contain different colored text,
adaptive thresholding methods work better and produce good results. As shown in Fig. \ref{fig:5}, adaptive thresholding
method may perform better than global one. However, in practice we note that a simpler global thresholding
scheme works well in most cases.

\subsection{Experimental Results and Analysis}

We used the ICDAR 2003 Robust Word Recognition Dataset \cite{A15} for our experiments.
The dataset contains a set of JPEG images of single words (Sample (171 words), 
TrialTrain (1157 words) and TrialTest (1111 words)). For qualitative evaluation,
we selected the word images that had complex reflective, shadowed and specular background.  
We separate these word images into Red, Green and Blue channels
assuming that these are the mixture images of the independent source images
that contains the foreground (text) and background. These three images are used
for extracting the foreground as described before.

\begin{table}[ht]
\centering
 \caption{Quantitative Results (Average)}
 \scalebox{1.3}{
  \begin{tabular}{| l | c | c | c |}
    \hline
    Method & Precision & Recall & F-score \\ \hline
    Otsu \cite{A2} & .68 & .75 & 69.17 \\ \hline
    Sauvola \cite{A6} & .63 & .81 & 66.94 \\ \hline
    Kittler \cite{A5} & .66 & .76 & 64.33 \\ \hline    
    Niblack \cite{A9} & .70 & .76 & 71.32 \\ \hline   
    MRF \cite{A16} & .79 & .86 & 80.38 \\ \hline
    Proposed &  .86 & .83 & 83.60\\ \hline
  \end{tabular}}
\label{fig:tab1}
\end{table}
\begin{table}[ht]
\centering
 \caption{OCR Accuracy (\%)}
 \scalebox{1.3}{
  \begin{tabular}{| l | c | c | c |}
    \hline
    Method & Word Accuracy\\ \hline
    MRF \cite{A16} & 43.2 \\ \hline
    Proposed &  61.6\\ \hline
  \end{tabular}}
\label{fig:tab2}
\end{table}

We compare the performance of our method with four well known thresholding algorithms 
i.e Kittler \cite{A5}, Otsu \cite{A2}, Niblack \cite{A9} and Sauvola \cite{A6}.
%In Fig \ref{fig:good}, we can see that these techniques perform fairly well when the 
%foreground text and background are well defined.
In the presence of complex
background, the segmentation accuracy of the thresholding based methods decreases (Fig \ref{fig:bad}).
We also compare with the recent method by Mishra \emph{et al} \cite{A16}.
It although performs well for many images but severely
fails in cases of shadows, high illumination variations 
in the image. This poor show is likely due to fact that
performance of the algorithm heavily depends on initial seeds.
We show both qualitative and quantitative results of the proposed method. 
\begin{figure}[p]
\centering
\subfigure[]{
\includegraphics[height=3.5in,width=5in]{chap4/res_7/pixel_grid.eps}
}
\label{fig:accuracy}
\caption
{Pixel Grid showing precision and recall in a word image}
\vspace{6mm}
\subfigure[]{
\includegraphics[height=3.5in,width=5in]{chap4/res_7/ocr_recog.eps}
}
\label{fig:ocr}
\caption
{OCR results on scene text images}
\end{figure}
\begin{figure*}[p]
\centering

\subfigure{
\includegraphics[height=.6in,width=1.8in]{results/res_2/1/orig.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/1/mrf.eps}
%\subfigure{
%\includegraphics[height=.55in,width=1.2in]{results/res_2/1/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.55in,width=1.2in]{results/res_2/1/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.55in,width=1.2in]{results/res_2/1/nib.eps}
%}
%\includegraphics[height=.7in,width=1.8in]{results/res_2/1/graph.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/1/res.eps}
}
%\label{fig:subfig11}
\subfigure{
\includegraphics[height=.6in,width=1.8in]{results/res_2/2/orig.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/2/mrf.eps}
%\subfigure{
%\includegraphics[height=.55in,width=1.2in]{results/res_2/2/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.55in,width=1.2in]{results/res_2/2/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.55in,width=1.2in]{results/res_2/2/nib.eps}
%}
%\includegraphics[height=.55in,width=1.3in]{results/res_2/2/graph.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/2/res.eps}
}
%\\
%\label{fig:subfig11}
\subfigure{
\includegraphics[height=.6in,width=1.8in]{results/res_2/3/orig.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/3/mrf.eps}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/3/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/3/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/3/nib.eps}
%}
%\includegraphics[height=.55in,width=1.3in]{results/res_2/3/graph.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/3/res.eps}
}
%\\
%\label{fig:subfig11}
\subfigure{
\includegraphics[height=.6in,width=1.8in]{results/res_2/12/orig.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/12/mrf.eps}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/12/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/12/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/12/nib.eps}
%}
%\includegraphics[height=.55in,width=1.3in]{results/res_2/12/graph.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/12/res.eps}
}
%\\
%\label{fig:subfig11}
%
%\caption
%{Comparison of Binarization algorithms and the proposed method 
%(From left to right Original, MRF, Kittler, Otsu, Niblack, Sauvola, Proposed)
%Text containing reflective background
%}
%\label{fig:6}
%\end{figure*}
%\begin{figure*}[t]
%\centering
%
\subfigure{
\includegraphics[height=.6in,width=1.8in]{results/res_2/5/orig.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/5/mrf.eps}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/5/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/5/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/5/nib.eps}
%}
%\includegraphics[height=.55in,width=1.3in]{results/res_2/5/graph.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/5/res.eps}
}
%\\
%\label{fig:subfig11}
\subfigure{
\includegraphics[height=.6in,width=1.8in]{results/res_2/6/orig.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/6/mrf.eps}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/6/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/6/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/6/nib.eps}
%}
%\includegraphics[height=.55in,width=1.3in]{results/res_2/6/graph.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/6/res.eps}
}
%\\
%\label{fig:subfig11}
%\caption
%{Comparison of Binarization algorithms and the proposed method 
%(From left to right Original, MRF, Kittler, Otsu, Niblack, Sauvola, Proposed)
%Text containing shadowed background
%}
%\label{fig:6}
%\end{figure*}
%\begin{figure*}[t]
%\centering
\subfigure{
\includegraphics[height=.6in,width=1.8in]{results/res_2/7/orig.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/7/mrf.eps}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/7/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/7/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/7/nib.eps}
%}
%\includegraphics[height=.55in,width=1.3in]{results/res_2/7/graph.eps}
\includegraphics[height=.7in,width=1.8in]{results/res_2/7/res.eps}
}
%\\
%\label{fig:subfig11}
\subfigure{
\includegraphics[height=.6in,width=1.8in]{results/res_2/8/orig.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/8/mrf.eps}
%\subfigure{
%\includegraphics[height=.4in,width=1in]{results/res_2/8/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.4in,width=1in]{results/res_2/8/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.4in,width=1in]{results/res_2/8/nib.eps}
%}
%\includegraphics[height=.7in,width=1.8in]{results/res_2/8/graph.eps}
\includegraphics[height=.7in,width=1.8in]{results/res_2/8/res.eps}
}
%\\
%\label{fig:subfig11}
\subfigure{
\includegraphics[height=.6in,width=1.8in]{results/res_2/9/orig.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/9/mrf.eps}
%\subfigure{
%\includegraphics[height=.4in,width=1in]{results/res_2/9/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.4in,width=1in]{results/res_2/9/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.4in,width=1in]{results/res_2/9/nib.eps}
%}
%\includegraphics[height=.7in,width=1.8in]{results/res_2/9/graph.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/9/res.eps}
}
%\\
%\label{fig:subfig11}
\subfigure{
\includegraphics[height=.6in,width=1.8in]{results/res_2/10/orig.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/10/mrf.eps}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/10/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/10/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/10/nib.eps}
%}
%\includegraphics[height=.55in,width=1.3in]{results/res_2/10/graph.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/10/res.eps}
}
%\\
%\label{fig:subfig11}
\subfigure{
\includegraphics[height=1in,width=1.8in]{results/res_2/11/orig.eps}
\includegraphics[height=1in,width=1.8in]{results/res_2/11/mrf.eps}
%\subfigure{
%\includegraphics[height=.7in,width=1in]{results/res_2/11/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.7in,width=1in]{results/res_2/11/otsu.eps}
%\includegraphics[height=.7in,width=1in]{results/res_2/11/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.7in,width=1in]{results/res_2/11/nib.eps}
%}
%\subfigure{
%\includegraphics[height=.7in,width=1.3in]{results/res_2/11/graph.eps}
\includegraphics[height=1in,width=1.8in]{results/res_2/11/res.eps}
}
\label{fig:subfig11}
\caption
{Comparison of Binarization algorithms and the proposed method 
(From left to right Original, MRF, Proposed)}
\label{fig:proposed}
\end{figure*}
We took around 50
images from the dataset and generated its ground truth images for pixel level accuracy.
Some of these images are shown in Fig \ref{fig:ground}.
We use well known measures like precision, recall and F-score (Fig 4.9) %\ref{fig:accuracy}) 
to compare the proposed method with different binarization methods (Table I).
We also use OCR accuracy to show the effectiveness of our method. Note that we are only
using the subset of images that are most degraded by shadowing, illumination variations,
noise and specular reflections. The results of thresholding schemes are too poor for the
OCR algorithm to give any output. Therefore we only compare with the recent MRF~\cite{A16}
based model as shown in Table II. Some OCR results are shown in Fig 4.10. %\ref{fig:ocr}.

\begin{figure*}[t]
\centering
\subfigure[]{
\includegraphics[height=.7in,width=2.2in]{results/res_7/1.eps}
}
\subfigure[]{
\includegraphics[height=.7in,width=2.2in]{results/res_7/2.eps}
}
\subfigure[]{
\includegraphics[height=.7in,width=2.2in]{results/res_7/3.eps}
}
\subfigure[]{
\includegraphics[height=.7in,width=2.2in]{results/res_7/4.eps}
}
\label{fig:text}
\caption
{(a) Image containing Text over another Text (b) Foreground Text (c) Background Text (d)
Text extracted}
\subfigure[]{
\includegraphics[height=.6in,width=1.7in]{results/res_6/1.eps}
\includegraphics[height=.6in,width=1.7in]{results/res_6/2.eps}
}
\subfigure[]{
\includegraphics[height=.6in,width=1.7in]{results/res_6/3.eps}
\includegraphics[height=.6in,width=1.7in]{results/res_6/4.eps}
\label{fig:subfig11}
}
\caption
{Failure case where (a) Both the foreground and background are
of same color (b) Different Colored Text}
\label{fig:failure}
\end{figure*}

The results show that the proposed method is an effective method and performs better than other methods 
in the case where images have
complex background. The qualitative results are shown in Fig. \ref{fig:proposed}. Fig 4.12 shows that our 
technique can
also be applied to images containing text over another text. 
We analyze that the above methods do not work in the case where there is a complex and textured background in the images.
It is not that these methods do not work at all. No single algorithm works well for all types of images. Thus we can say
that our method can extract out the text embedded in complex reflective, shadowed and
specular background.
%Though there are some exceptions. Our method does not produce good results when the foreground text and the
%background are of the same color as shown in Fig. \ref{fig:8}.
The failure case of our method is shown in Fig. \ref{fig:failure}.
Our method fails in cases where foreground text and the background are of the same color
and cases where text is composed of different colored letters.
Moreover, the approach works only with color images.

% In this section, the results of the proposed method are
% presented. We used ICDAR Robust Word Recognition Dataset \cite{A15}
% for experiments. The dataset contains a set of JPEG images of single words
% (Sample (171 words), TrialTrain (1157 words) and TrialTest (1111 words)).
% We work on the word images that had complex reflective, shadowed and specular background.  
% First, we separate these word images into Red, Green and Blue channels
% assuming that these are the mixture images of the independent source images
% that contains the foreground (text) and background. 
% Then we apply ICA technique to get the independent source images.
% Finally, global thresholding is applied on the component, having maximum information about the foreground text,
% to get the binarize text image.
% 
% We compare the performance of our method with four
% well known thresholding algorithms i.e Kittler \cite{A5},
% Otsu \cite{A2}, Niblack \cite{A9} and Sauvola \cite{A6}. 
% These methods fail to deal with the challenges of scene text images like 
% reflections, shadows and specularities.
% We also compare with the recent method by Mishra \emph{et al} \cite{A16}.
% It although performs well for many images but severely
% fails in cases of shadows, high illumination variations 
% in the image. This poor show is likely due to fact that
% performance of the algorithm heavily depends on initial seeds. 
% 
% 
% We believe a visual evaluation is sufficient for a qualitative estimation of
% our method.
% The results show that the proposed method is an effective
% method and performs better than other methods in the case where images have
% complex background. Fig. \ref{fig:7} shows that our technique can also be applied to text image
% containing two different types of colored text.

%We show both qualitative and quantative results of the proposed method. 
%The qualitative results are shown in Fig. \ref{fig:6}.
%We also use OCR accuracy to show the effectiveness of our method. 
%OCR accuracy for the well known thresholding methods are poor. Therefore
%we only compare with the recent MRF \cite{A16} based model.

%\begin{table}[ht]
%\centering
% \caption{OCR Accuracy (\%)}
% \scalebox{1.3}{
%  \begin{tabular}{| l | c | c | c |}
%    \hline
%    Method & Character Accuracy\\ \hline
%    MRF \cite{A16} & 29.2 \\ \hline
%    Proposed &  50.7\\ \hline
%  \end{tabular}}
%\end{table}


% We analyze that the above methods do not work in the case 
% where there is a complex and textured background in the images.
% It is not that these methods do not work at all. No single algorithm works well for
% all types of images. 
% Thus we can say that our method can extract out the text embedded in complex reflective, shadowed and
% specular background. The failure case of our method is shown in Fig. \ref{fig:8}.
% Our method fails in cases where foreground text and the background are of the same color.



%\begin{figure}[t]
%\centering

%\label{fig:8}
%\end{figure}

\section{Applications}

\subsection{Inscribed Text Segmentation}

Inscribed text is difficult to extract from one image as both the foreground and the background is of same 
color.
The inscriptions are generally found engraved/carved into stones, marble, metal or wood.
Extracted of the text is important as we want to preserve our historical writings which are currently being decayed.
However, due to effects of uncontrolled lighting condition and degradation of the material of the inscribed text,
extracting text from these images has become challenging problem. Fig \ref{fig:inscribe} shows the inscribed text images.
Hence we propose an efficient technique which can solve this problem.
\begin{figure}[t]
\centering
\subfigure{
\includegraphics[height=1.5in,width=1.8in]{chap4/res_4/try.eps}
}
\subfigure{
\includegraphics[height=1.5in,width=1.8in]{chap4/res_3/orig.eps}
}
\label{fig:subfig11}
\caption
{Inscribed Text image where both background and foreground are of same color}
\label{fig:inscribe}
%\end{figure}
%\begin{figure}[htbp]
%\vspace{10mm}
\centering
\subfigure[]{
\includegraphics[height=1.5in,width=1.8in]{chap4/res_2/orig.eps}
}
\subfigure[]{
\includegraphics[height=1.5in,width=1.8in]{chap4/res_2/direct.eps}
}
\subfigure[]{
\includegraphics[height=1.5in,width=1.8in]{chap4/res_2/global.eps}
}
\label{fig:subfig11}
\caption
{(a) Sponge Texture (b),(c) Independent components}
\label{fig:IC}
\end{figure}
\begin{figure}[h]
\centering
\subfigure[]{
\includegraphics[height=1.5in,width=1.8in]{chap4/res_1/orig.eps}
}
\subfigure[]{
\includegraphics[height=1.5in,width=1.8in]{chap4/res_1/ica_1.eps}
}
\subfigure[]{
\includegraphics[height=1.5in,width=1.8in]{chap4/res_1/ica_2.eps}
\label{fig:subfig11}
}
\caption
{(a) Image containing text (b),(c) Independent components}
\label{fig:ictext}
\end{figure}
\begin{figure}[p]
\centering
\subfigure[Original text ]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_3/orig.eps}
}
\subfigure[Kittler]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_3/kit.eps}
}
\subfigure[Otsu]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_3/otsu.eps}
}
\subfigure[Niblack]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_3/nib.eps}
}
\subfigure[Sauvola]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_3/sau.eps}
}
\subfigure[ICA + CBM]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_3/proposed.eps}
}
%\includegraphics[height=.5in,width=1.6in]{chap4/res_1/ica_2.eps}
\label{fig:res1}
\caption
{Binarized text}
%\end{figure}
%\begin{figure}[t]
\centering
\subfigure[Original text ]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_6/orig.eps}
}
\subfigure[Kittler]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_6/kit.eps}
}
\subfigure[Otsu]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_6/otsu.eps}
}
\subfigure[Niblack]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_6/nib.eps}
}
\subfigure[Sauvola]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_6/sau.eps}
}
\subfigure[ICA + CBM]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_6/proposed.eps}
}
%\includegraphics[height=.5in,width=1.6in]{chap4/res_1/ica_2.eps}
\label{fig:res2}
\caption
{Binarized text}
\end{figure}
First we capture multiple images of the inscribed text and apply our model to extract the text with the help of shadows.
For separating the global and the direct component, we used high frequency checkerboard pattern. 
This takes too much time as we have to capture many images.(Figure \ref{fig:IC} shows
the independent component of the sponge texture)
But for this, we apply an Independent Component Analysis (ICA) based method to the images captured containing text.
This method helps in extracting out the shadows (Figure \ref{fig:ictext}). 
Then we apply our component based model to efficiently binarize the text
embedded (Figure 4.17, 4.18).

\subsection{Enhancing Edge Detection}



Edge/Boundary detection is a fundamental task in computer vision with its applications 
in areas such as image segmentation, object recognition, object description, feature detection and extraction. 
It refers to the process of identifying points in an image where there are sharp discontinuities or
the image brightness changes sharply.
These points are ordered into a set of curved line segments known as edges.
The abrupt changes in pixel intensity values characterizes boundaries of objects in a scene.
The amount of data to be processed significantly reduces when we apply an edge detection algorithm.
This helps in filtering out data which is less important and still preserving the structural
properties of the image.
The task of interpreting the important information content in the original 
image may be simplified if the edge detection step is successful. However,
due to complexities in real scene images, it is not always possible to obtain 
clear edges which define the boundary of the object. Edge detection process is particularly sensitive to
noise and uneven illumination conditions. There are many edge detection algorithms available.
Each are sensitive to certain types of edges.
\begin{figure*}[t]
\centering
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/15.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/8.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/1.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/16.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/9.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/2.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/17.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/10.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/4.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/18.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/11.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/5.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/21.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/14.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/7.eps}
}
%\label{fig:subfig11}
\caption
{From left to right: Text image, Applying canny edge detector, Applying ICA model + canny edge detector}
\label{fig:edge}
\end{figure*}
Many models \cite{chap4-6, chap4-7} use edge detection as the first step 
to extract text from scene images. But the performance of this step decreases when 
complex background is present in the image.  
Here we will show how we use our ICA model to correctly identify the word boundaries.
First we use canny edge detector \cite{chap4-5} to detect edges from natural scene text images. 
When we directly apply canny edge detector on scene text images, we observe discontinuity in word boundaries.
But when we use our ICA model followed the canny edge detector, the edge boundaries becomes much more cleaner.
Fig \ref{fig:edge} shows the effectiveness of our approach.


% 
% Boundary detection constitutes a crucial step in many computer vision tasks. A
% boundary map of an image can provide valuable information for further image
% analysis and interpretation tasks such as segmentation, object description etc.
% Fig. 1 shows an image and the associated boundary map as marked by human
% observers. It can be noted that the map essentially retains gross but important
% details in the image. It is hence sparse yet rich in information from the point
% of scene understanding. Extracting a similar boundary map is of interest in
% computer vision
% 
% Now edges are to be detected from the preprocessed image. 
% Conventional edge detection algorithms belong to the high 
% pass filtering, which are not fit for complex background 
% images. Fixed global threshold will usually work well for 
% images with uniform background, but not for textured 
% background and the contrast of objects varies within the image 
% .In such cases, it is convenient to use a threshold gray level 
% that is a slowly varying function of position in the image[14]. 
% And also edge continuity to be maintained and edge 
% overlapping to be avoided. In this aspect, an iterative 
% thresholding algorithm and morphologic erode algorithm [15] 
% are used here to detect the edges. Following steps are used to 
% detect edges in iterative thresholding
%http://cvit.iiit.ac.in/papers/gopal06AComputational.pdf
%http://vision.ece.ucsb.edu/publications/00edgeflow.pdf
%http://www.cs.cmu.edu/~rahuls/pub/eccv2012-rahuls.pdf


\begin{figure*}[t]
\centering
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/1.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/2.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/3.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/4.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/5.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/6.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/7.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/8.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/9.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/10.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/11.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/12.eps}
}
%\label{fig:subfig11}
\caption
{Scene images containing shadows and their corresponding shadow masks}
\label{fig:shadow}
\end{figure*}

\subsection{Shadow Detection}
Shadows are generally created when objects obscure the light source. 
Detected shadows provide information about
lighting direction and scene geometry.
Due to the presence of shadows many computer vision 
tasks such as object detection, segmentation, scene analysis, tracking, etc are affected.
For example, the shadows may assign false segments in the 
image segmentation process. They may be wrongly detected as objects in 
object detection algorithms.
For these reasons,
shadow detection is an important component in scene interpretation.
Hence 
shadow detection and removal is an important preprocessing step for improving the performance of such vision tasks.
But decomposition of the image into a shadow image and a non-shadow image
is a difficult problem due to 
complex interactions of geometry and illumination.
Various pixel-based and region-based methods are
proposed to detect the shadows in an image \cite{chap4-8, chap4-9, chap4-10, chap4-11}.
Here we show that our ICA model can be used to detect shadows and create shadow masks from the scene images.
Fig \ref{fig:shadow} shows the shadow detected results. We first apply our ICA model followed by
global binarization method. Our method does not give good results if some dark objects are present in the scene
images which are mistaken as shadows.
%our goal is to create a system that can automatically detect shadow regions and then remove from simple images. 
%While detecting all shadows is expected to remain hard, we make some constraints related to the structure of the images. 
%http://www.cs.illinois.edu/~dhoiem/publications/cvpr11_shadow.pdf
%http://www.cit.iit.bas.bg/CIT_2013/v13-1/S_Murali,%20V_Govindan.pdf

